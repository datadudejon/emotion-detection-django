# -*- coding: utf-8 -*-
"""Emotion_recognition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v7ryUiyrv6kSPFyALUybF4pyb9Z7bVPk
"""

import kagglehub
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os

# Download latest version
path = kagglehub.dataset_download("msambare/fer2013")

print("Path to dataset files:", path)

# Path to train directory
train_path = os.path.join(path, 'train')
# Path to test directory
test_path = os.path.join(path, 'test')

# Define core parameters
BATCH_SIZE = 32
IMG_HEIGHT = 48
IMG_WIDTH = 48

# Load train Datasets
train_ds = tf.keras.utils.image_dataset_from_directory(
    train_path,
    validation_split=0.2,
    subset='training',
    seed=123,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    color_mode='grayscale'
)

# Load validation set
val_ds = tf.keras.utils.image_dataset_from_directory(
    train_path,
    validation_split=0.2,
    subset='validation',
    seed=123,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    color_mode='grayscale'
)

test_ds = tf.keras.utils.image_dataset_from_directory(
    test_path,
    seed=123,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    color_mode='grayscale'
)

# Optimized data pipeline
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
validation_dataset = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_dataset = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

# Extract class names and class length
class_names= train_ds.class_names
num_classes = len(class_names)
print(class_names)

# Define the model
model = keras.Sequential([
    # Input and normalization layer
    layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 1)),

    # Feature extraction layer
    # Block 1
    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
    layers.MaxPooling2D((2, 2)),

    # Block 2
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.MaxPooling2D((2, 2)),

    # Block 3
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.MaxPooling2D((2, 2)),

    # The Classifier
    # Converts the 2D image data to a 1D list befpre feeding to the standard/dense layer
    layers.Flatten(),

    # Hidden layer with 128 neaurons
    layers.Dense(128, activation='relu'),

    # Dropout to avoid overfitting
    layers.Dropout(0.5),

    # Output layer
    layers.Dense(num_classes, activation='softmax')
]

)

# Brings all the pieces together and creates the architecture
model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy']
)

model.summary()

# TRAIN MODEL
# Sets number of passes
EPOCHS = 15

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS
)

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(EPOCHS)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

print("Evaluating model on the test dataset...")
test_loss, test_acc = model.evaluate(test_ds)
print(f"\nTest Accuracy: {test_acc*100:.2f}%")

# Save model
model.save('ENYI_EMOTION_MODEL.keras')

print("\nModel saved as 'ENYI_EMOTION_MODEL.keras'")

